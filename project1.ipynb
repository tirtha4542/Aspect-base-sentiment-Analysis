{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.780489Z",
     "start_time": "2026-02-11T13:40:14.777286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import  torch\n",
    "import re\n",
    "\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "from torch.special import logit\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch import optim\n",
    "from torch.xpu import device\n",
    "from torchmetrics.classification import Accuracy\n",
    "import torch.nn as nn\n",
    "from torchmetrics.functional import accuracy\n"
   ],
   "id": "cd53b5d99773f176",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.793690Z",
     "start_time": "2026-02-11T13:40:14.787388Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device('cuda' if torch.cuda.is_available()else 'cpu')",
   "id": "334222e8d8ccbd8",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.799411Z",
     "start_time": "2026-02-11T13:40:14.797348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "ec08110ac8f9b0e9",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.804752Z",
     "start_time": "2026-02-11T13:40:14.802805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_path =r\"E:\\capstone_project_1\\data\\train (2).csv\"\n",
    "test_path = r\"E:\\capstone_project_1\\data\\test (2).csv\""
   ],
   "id": "1728fbb6b21a0e9",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.819323Z",
     "start_time": "2026-02-11T13:40:14.809328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ],
   "id": "edafc2ed70c8a7b8",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.825158Z",
     "start_time": "2026-02-11T13:40:14.823060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]','',text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n"
   ],
   "id": "38f6c7519258d2eb",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.833492Z",
     "start_time": "2026-02-11T13:40:14.829820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = test_df.iloc[4]['review']\n",
    "normalize(text)"
   ],
   "id": "2f693d1556fa3554",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certainly not the best sushi in new york however it is always fresh and the place is very clean sterile'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.840607Z",
     "start_time": "2026-02-11T13:40:14.838546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n"
   ],
   "id": "5daa72154fb336d2",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.847425Z",
     "start_time": "2026-02-11T13:40:14.845102Z"
    }
   },
   "cell_type": "code",
   "source": "tokeni_tex = tokenize(text)",
   "id": "536e5a9448edc682",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.868649Z",
     "start_time": "2026-02-11T13:40:14.850692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_2_id = {\n",
    "        '<PAD>':0,\n",
    "        '<UNK>':1\n",
    "    }\n",
    "corpus = train_df[\"review\"].tolist()\n",
    "idx = 2\n",
    "for text in corpus:\n",
    "    text = normalize(text)\n",
    "    token = tokenize(text)\n",
    "    for token in token:\n",
    "        if token not in token_2_id:\n",
    "            token_2_id[token] = idx\n",
    "            idx += 1"
   ],
   "id": "fe2bce5293224ddf",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.877054Z",
     "start_time": "2026-02-11T13:40:14.874078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = [token_2_id.get(token,token_2_id['<UNK>']) for token in tokeni_tex]\n",
    "input"
   ],
   "id": "cdcea4afb75f4304",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 52,\n",
       " 3,\n",
       " 151,\n",
       " 249,\n",
       " 72,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 962,\n",
       " 1,\n",
       " 47,\n",
       " 30,\n",
       " 393,\n",
       " 644,\n",
       " 1,\n",
       " 63,\n",
       " 3,\n",
       " 223,\n",
       " 30,\n",
       " 35,\n",
       " 744,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.883968Z",
     "start_time": "2026-02-11T13:40:14.881387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_vocab(text):\n",
    "    token_2_id = {\n",
    "        '<PAD>':0,\n",
    "        '<UNK>':1\n",
    "    }\n",
    "    corpus = train_df[\"review\"].tolist()\n",
    "    idx = 2\n",
    "    for text in corpus:\n",
    "        text = normalize(text)\n",
    "        token = tokenize(text)\n",
    "        for token in token:\n",
    "            if token not in token_2_id:\n",
    "                token_2_id[token] = idx\n",
    "                idx += 1\n",
    "    return token_2_id"
   ],
   "id": "1cb56477867145df",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.892682Z",
     "start_time": "2026-02-11T13:40:14.890071Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.columns",
   "id": "d535356abd28384",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'aspect', 'sentiment'], dtype='str')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.901900Z",
     "start_time": "2026-02-11T13:40:14.898585Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.columns",
   "id": "aab8fde658f6fa77",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'aspect', 'sentiment'], dtype='str')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.907246Z",
     "start_time": "2026-02-11T13:40:14.905358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_map = {\n",
    "    \"negative\":0,\n",
    "    \"positive\":1,\n",
    "    \"neutral\":2,\n",
    "\n",
    "\n",
    "}"
   ],
   "id": "a47241d56856ce5c",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.916069Z",
     "start_time": "2026-02-11T13:40:14.912493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ABCDataset(Dataset):\n",
    "    def __init__(self,df,token_2_id,label_map):\n",
    "        self.df = df\n",
    "        self.token_2_id = token_2_id\n",
    "        self.label_mal = label_map\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.df.iloc[idx]\n",
    "        text = example[\"review\"]\n",
    "        aspect = example[\"aspect\"]\n",
    "        sentiment =example[\"sentiment\"]\n",
    "        text_aspact_pair = text +  \" \" + aspect\n",
    "        normalize_text = normalize(text_aspact_pair)\n",
    "        tokens = tokenize(normalize_text)\n",
    "        input_ids = [self.token_2_id.get(token,self.token_2_id[\"<UNK>\"]) for token in tokens]\n",
    "        label_id = self.label_mal[sentiment]\n",
    "        return {\n",
    "            \"input_ids\":input_ids,\n",
    "            \"label_ids\":label_id\n",
    "        }\n"
   ],
   "id": "1a8ace07539fec29",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:02:48.256549Z",
     "start_time": "2026-02-11T14:02:48.251426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ABSADataModule(pl.LightningDataModule):\n",
    "    def __init__(self,train_path,test_path,batch_size):\n",
    "        super().__init__()\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.batch_size = batch_size\n",
    "    def setup(self,stage=None):\n",
    "        train_df = pd.read_csv(self.train_path)\n",
    "\n",
    "        test_df = pd.read_csv(self.test_path)\n",
    "        self.token_2_id = build_vocab(train_df['review'])\n",
    "        self.label_map = label_map\n",
    "        self.train_set = ABCDataset(train_df,self.token_2_id,self.label_map)\n",
    "        self.test_set = ABCDataset(test_df,self.token_2_id, self.label_map)\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.collate_fn)\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.collate_fn)\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        batch_input_ids = [item['input_ids']for item in batch]\n",
    "        batch_labels = [item['label_ids']for item in batch]\n",
    "        max_len = max(len(input_ids) for input_ids in batch_input_ids)\n",
    "        pad_token_id = self.token_2_id[\"<PAD>\"]\n",
    "\n",
    "        batch_padded_input_ids = [\n",
    "            input_ids + [pad_token_id] * (max_len-len(input_ids))for input_ids in batch_input_ids\n",
    "        ]\n",
    "        return {\n",
    "            \"batch_input_ids\":torch.tensor(batch_padded_input_ids,dtype=torch.long),\n",
    "            \"batch_label\":torch.tensor(batch_labels,dtype=torch.long)\n",
    "        }"
   ],
   "id": "6173ba4f15cd2460",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:40:14.929484Z",
     "start_time": "2026-02-11T13:40:14.928071Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3e49bc2a08ae5c66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:02:49.882666Z",
     "start_time": "2026-02-11T14:02:49.858568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_module = ABSADataModule(\n",
    "    train_path = train_path,\n",
    "    test_path = test_path,\n",
    "\n",
    "    batch_size=32\n",
    ")\n",
    "data_module.setup()"
   ],
   "id": "fbf61369d80bb50",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:02:51.261666Z",
     "start_time": "2026-02-11T14:02:51.258352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ABSA(nn.Module):\n",
    "    def __init__(self,vocab_size,num_labels = 3):\n",
    "        super(ABSA,self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size,embedding_dim=256)\n",
    "        self.lstm_layer = nn.LSTM(input_size=256,hidden_size=512,batch_first=True)\n",
    "        self.fc_layer = nn.Linear(in_features=512,out_features=num_labels)\n",
    "    def forward(self,x):\n",
    "        embeddings = self.embedding_layer(x)\n",
    "        lstm_out,_ = self.lstm_layer(embeddings)\n",
    "        logits = self.fc_layer(lstm_out[:,-1,:])\n",
    "        return logits"
   ],
   "id": "4cddd5cb5f86cf8a",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:08:41.056228Z",
     "start_time": "2026-02-11T14:08:41.051737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ABSAModel(pl.LightningModule):\n",
    "    def __init__(self,vocab_size,num_labels=3):\n",
    "        super().__init__()\n",
    "        self.model = ABSA(vocab_size,num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.num_labels = num_labels\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        input_ids = batch[\"batch_input_ids\"].to(self.device)\n",
    "        labels = batch['batch_label'].to(self.device)\n",
    "        logits = self.forward(input_ids)\n",
    "        loss = self.loss_fn(logits,labels)\n",
    "        self.log(\"train_loss:\",loss,prog_bar=True)\n",
    "        acc = self.compute_metrics(logits,labels)\n",
    "        self.log(\"train_acc:\",acc,prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        input_ids = batch[\"batch_input_ids\"].to(self.device)\n",
    "        labels = batch['batch_label'].to(self.device)\n",
    "        logits = self.forward(input_ids)\n",
    "        loss = self.loss_fn(logits,labels)\n",
    "        self.log(\"test_loss:\",loss,prog_bar=True)\n",
    "        acc = self.compute_metrics(logits,labels)\n",
    "        self.log(\"test_acc:\",acc,prog_bar=True)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(),lr = 3e-4)\n",
    "    def compute_metrics(self,logits,labels):\n",
    "        preds = torch.argmax(logits,dim=1)\n",
    "        accuracy = Accuracy(task=\"multiclass\",num_classes=self.num_labels)\n",
    "        return accuracy(preds.cpu(),labels.cpu())\n",
    "\n"
   ],
   "id": "3d38e109bf0aa938",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:08:43.182689Z",
     "start_time": "2026-02-11T14:08:43.170846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = ABSAModel(\n",
    "    vocab_size=len(data_module.token_2_id),\n",
    "    num_labels=3\n",
    ").to(device)"
   ],
   "id": "a48a90274ab39a40",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:11:26.538816Z",
     "start_time": "2026-02-11T14:11:26.525545Z"
    }
   },
   "cell_type": "code",
   "source": "trainer = pl.Trainer(max_epochs=3)",
   "id": "5e4220be4aff0f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:11:51.672558Z",
     "start_time": "2026-02-11T14:11:29.123411Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.fit(model,data_module)",
   "id": "7a79574c4614e29c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\n",
      "  | Name    | Type             | Params | Mode  | FLOPs\n",
      "-------------------------------------------------------------\n",
      "0 | model   | ABSA             | 2.6 M  | train | 0    \n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train | 0    \n",
      "-------------------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.205    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:07<00:00, 14.68it/s, v_num=12, train_loss:=0.858, train_acc:=0.708]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:07<00:00, 14.50it/s, v_num=12, train_loss:=0.858, train_acc:=0.708]\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T16:30:10.878884Z",
     "start_time": "2026-02-11T16:30:10.251826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.test(\n",
    "    model,\n",
    "    data_module\n",
    ")\n"
   ],
   "id": "7695278a36febf7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 59.13it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n",
      "       Test metric             DataLoader 0\r\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n",
      "        test_acc:            0.644325315952301\r\n",
      "       test_loss:           0.8298655152320862\r\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss:': 0.8298655152320862, 'test_acc:': 0.644325315952301}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T16:30:18.539645Z",
     "start_time": "2026-02-11T16:30:18.529705Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.model.state_dict(),\"model_weights.pth\")",
   "id": "98ddc7e2ac3b107d",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T16:32:59.065397Z",
     "start_time": "2026-02-11T16:32:58.438255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "score = trainer.test(\n",
    "    model,\n",
    "    data_module\n",
    ")"
   ],
   "id": "a43a57349652ed88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 59.12it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n",
      "       Test metric             DataLoader 0\r\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n",
      "        test_acc:            0.644325315952301\r\n",
      "       test_loss:           0.8298655152320862\r\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T16:37:40.574520Z",
     "start_time": "2026-02-11T16:37:40.571268Z"
    }
   },
   "cell_type": "code",
   "source": "score[0][\"test_acc:\"]",
   "id": "9d3d2a51868697bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.644325315952301"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T16:38:57.755029Z",
     "start_time": "2026-02-11T16:38:57.752041Z"
    }
   },
   "cell_type": "code",
   "source": "import mlflow as ml",
   "id": "aadfeea3d4337bcc",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T16:39:42.376352Z",
     "start_time": "2026-02-11T16:39:42.368661Z"
    }
   },
   "cell_type": "code",
   "source": "ml.set_experiment(experiment_name=\"project1\")",
   "id": "8b2e50c0072b7e8d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:E:/capstone_project_1/mlruns/1', creation_time=1770827969584, experiment_id='1', last_update_time=1770827969584, lifecycle_stage='active', name='project1', tags={}>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:11:21.956154Z",
     "start_time": "2026-02-11T17:11:14.567163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Setup Tracking URI and Experiment\n",
    "# Using an absolute path ensures the UI always finds your data\n",
    "tracking_uri = f\"file:///{os.path.abspath('mlruns')}\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"ABSA_Sentiment_Analysis\")\n",
    "\n",
    "# 2. Setup Lightning Logger\n",
    "mlf_logger = pl.loggers.MLFlowLogger(\n",
    "    experiment_name=\"ABSA_Sentiment_Analysis\",\n",
    "    tracking_uri=tracking_uri\n",
    ")\n",
    "\n",
    "# 3. Execution Block\n",
    "with mlflow.start_run() as run:\n",
    "    # Log manual parameters\n",
    "    mlflow.log_params({\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 3,\n",
    "        \"vocab_size\": len(data_module.token_2_id)\n",
    "    })\n",
    "\n",
    "    # Initialize Trainer with the logger\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=3,\n",
    "        logger=mlf_logger,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    # Test\n",
    "    score = trainer.test(model, data_module)\n",
    "\n",
    "    # Log Metrics from test results\n",
    "    if score:\n",
    "        mlflow.log_metric(\"final_test_acc\", score[0].get(\"test_acc\", 0))\n",
    "        mlflow.log_metric(\"final_test_loss\", score[0].get(\"test_loss\", 0))\n",
    "\n",
    "    # 4. Handle Sample Batch (Fixed dictionary conversion)\n",
    "    test_loader = data_module.test_dataloader()\n",
    "    sample_batch = next(iter(test_loader))\n",
    "\n",
    "    # Move tensors to CPU and convert to Numpy\n",
    "    sample_numpy = {\n",
    "        k: v.cpu().numpy() if isinstance(v, torch.Tensor) else v\n",
    "        for k, v in sample_batch.items()\n",
    "    }\n",
    "\n",
    "    # Log the Model itself to MLflow\n",
    "    mlflow.pytorch.log_model(model.model, \"absa_lstm_model\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"To open the UI, run the following command in your terminal:\")\n",
    "    print(f\"mlflow ui --backend-store-uri {mlflow.get_tracking_uri()}\")"
   ],
   "id": "1f3dd635900abf6b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\n",
      "  | Name    | Type             | Params | Mode  | FLOPs\n",
      "-------------------------------------------------------------\n",
      "0 | model   | ABSA             | 2.6 M  | train | 0    \n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train | 0    \n",
      "-------------------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.205    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:06<00:00, 16.22it/s, v_num=6e81, train_loss:=0.246, train_acc:=0.958]"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 161] The specified path is invalid: '///E:/'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[103]\u001B[39m\u001B[32m, line 35\u001B[39m\n\u001B[32m     28\u001B[39m trainer = pl.Trainer(\n\u001B[32m     29\u001B[39m     max_epochs=\u001B[32m3\u001B[39m,\n\u001B[32m     30\u001B[39m     logger=mlf_logger,\n\u001B[32m     31\u001B[39m     log_every_n_steps=\u001B[32m10\u001B[39m\n\u001B[32m     32\u001B[39m )\n\u001B[32m     34\u001B[39m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_module\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     37\u001B[39m \u001B[38;5;66;03m# Test\u001B[39;00m\n\u001B[32m     38\u001B[39m score = trainer.test(model, data_module)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:584\u001B[39m, in \u001B[36mTrainer.fit\u001B[39m\u001B[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001B[39m\n\u001B[32m    582\u001B[39m \u001B[38;5;28mself\u001B[39m.training = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    583\u001B[39m \u001B[38;5;28mself\u001B[39m.should_stop = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m584\u001B[39m \u001B[43mcall\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    585\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    586\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    587\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    588\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    589\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    590\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    591\u001B[39m \u001B[43m    \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    592\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    593\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:49\u001B[39m, in \u001B[36m_call_and_handle_interrupt\u001B[39m\u001B[34m(trainer, trainer_fn, *args, **kwargs)\u001B[39m\n\u001B[32m     47\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m trainer.strategy.launcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     48\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[32m     52\u001B[39m     _call_teardown_hook(trainer)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:630\u001B[39m, in \u001B[36mTrainer._fit_impl\u001B[39m\u001B[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001B[39m\n\u001B[32m    623\u001B[39m     download_model_from_registry(ckpt_path, \u001B[38;5;28mself\u001B[39m)\n\u001B[32m    624\u001B[39m ckpt_path = \u001B[38;5;28mself\u001B[39m._checkpoint_connector._select_ckpt_path(\n\u001B[32m    625\u001B[39m     \u001B[38;5;28mself\u001B[39m.state.fn,\n\u001B[32m    626\u001B[39m     ckpt_path,\n\u001B[32m    627\u001B[39m     model_provided=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    628\u001B[39m     model_connected=\u001B[38;5;28mself\u001B[39m.lightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    629\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m630\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    632\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.state.stopped\n\u001B[32m    633\u001B[39m \u001B[38;5;28mself\u001B[39m.training = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1079\u001B[39m, in \u001B[36mTrainer._run\u001B[39m\u001B[34m(self, model, ckpt_path, weights_only)\u001B[39m\n\u001B[32m   1074\u001B[39m \u001B[38;5;28mself\u001B[39m._signal_connector.register_signal_handlers()\n\u001B[32m   1076\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m   1077\u001B[39m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[32m   1078\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1079\u001B[39m results = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1081\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m   1082\u001B[39m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[32m   1083\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m   1084\u001B[39m log.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: trainer tearing down\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1123\u001B[39m, in \u001B[36mTrainer._run_stage\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1121\u001B[39m         \u001B[38;5;28mself\u001B[39m._run_sanity_check()\n\u001B[32m   1122\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m torch.autograd.set_detect_anomaly(\u001B[38;5;28mself\u001B[39m._detect_anomaly):\n\u001B[32m-> \u001B[39m\u001B[32m1123\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfit_loop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1124\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1125\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnexpected state \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.state\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:218\u001B[39m, in \u001B[36m_FitLoop.run\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    216\u001B[39m     \u001B[38;5;28mself\u001B[39m.on_advance_start()\n\u001B[32m    217\u001B[39m     \u001B[38;5;28mself\u001B[39m.advance()\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mon_advance_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    220\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:480\u001B[39m, in \u001B[36m_FitLoop.on_advance_end\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    478\u001B[39m call._call_callback_hooks(trainer, \u001B[33m\"\u001B[39m\u001B[33mon_train_epoch_end\u001B[39m\u001B[33m\"\u001B[39m, monitoring_callbacks=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    479\u001B[39m call._call_lightning_module_hook(trainer, \u001B[33m\"\u001B[39m\u001B[33mon_train_epoch_end\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m480\u001B[39m \u001B[43mcall\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_call_callback_hooks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mon_train_epoch_end\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmonitoring_callbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    482\u001B[39m trainer._logger_connector.on_epoch_end()\n\u001B[32m    484\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.restarting \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.epoch_loop._num_ready_batches_reached():\n\u001B[32m    485\u001B[39m     \u001B[38;5;66;03m# since metric-based schedulers require access to metrics and those are not currently saved in the\u001B[39;00m\n\u001B[32m    486\u001B[39m     \u001B[38;5;66;03m# checkpoint, the plateau schedulers shouldn't be updated\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:228\u001B[39m, in \u001B[36m_call_callback_hooks\u001B[39m\u001B[34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001B[39m\n\u001B[32m    226\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(fn):\n\u001B[32m    227\u001B[39m         \u001B[38;5;28;01mwith\u001B[39;00m trainer.profiler.profile(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[Callback]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcallback.state_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m228\u001B[39m             \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlightning_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    230\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m pl_module:\n\u001B[32m    231\u001B[39m     \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[32m    232\u001B[39m     pl_module._current_fx_name = prev_fx_name\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:493\u001B[39m, in \u001B[36mModelCheckpoint.on_train_epoch_end\u001B[39m\u001B[34m(self, trainer, pl_module)\u001B[39m\n\u001B[32m    491\u001B[39m monitor_candidates = \u001B[38;5;28mself\u001B[39m._monitor_candidates(trainer)\n\u001B[32m    492\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._every_n_epochs >= \u001B[32m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m (trainer.current_epoch + \u001B[32m1\u001B[39m) % \u001B[38;5;28mself\u001B[39m._every_n_epochs == \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m493\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_save_topk_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmonitor_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    494\u001B[39m \u001B[38;5;66;03m# Only save last checkpoint if a checkpoint was actually saved in this step or if save_last=\"link\"\u001B[39;00m\n\u001B[32m    495\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._last_global_step_saved == trainer.global_step \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m    496\u001B[39m     \u001B[38;5;28mself\u001B[39m.save_last == \u001B[33m\"\u001B[39m\u001B[33mlink\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._last_checkpoint_saved\n\u001B[32m    497\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:591\u001B[39m, in \u001B[36mModelCheckpoint._save_topk_checkpoint\u001B[39m\u001B[34m(self, trainer, monitor_candidates)\u001B[39m\n\u001B[32m    589\u001B[39m     \u001B[38;5;28mself\u001B[39m._save_monitor_checkpoint(trainer, monitor_candidates)\n\u001B[32m    590\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m591\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_save_none_monitor_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmonitor_candidates\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:942\u001B[39m, in \u001B[36mModelCheckpoint._save_none_monitor_checkpoint\u001B[39m\u001B[34m(self, trainer, monitor_candidates)\u001B[39m\n\u001B[32m    940\u001B[39m \u001B[38;5;66;03m# set the best model path before saving because it will be part of the state.\u001B[39;00m\n\u001B[32m    941\u001B[39m previous, \u001B[38;5;28mself\u001B[39m.best_model_path = \u001B[38;5;28mself\u001B[39m.best_model_path, filepath\n\u001B[32m--> \u001B[39m\u001B[32m942\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_save_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.save_top_k == \u001B[32m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m previous \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_remove_checkpoint(trainer, previous, filepath):\n\u001B[32m    945\u001B[39m     \u001B[38;5;28mself\u001B[39m._remove_checkpoint(trainer, previous)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:600\u001B[39m, in \u001B[36mModelCheckpoint._save_checkpoint\u001B[39m\u001B[34m(self, trainer, filepath)\u001B[39m\n\u001B[32m    593\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_save_checkpoint\u001B[39m(\u001B[38;5;28mself\u001B[39m, trainer: \u001B[33m\"\u001B[39m\u001B[33mpl.Trainer\u001B[39m\u001B[33m\"\u001B[39m, filepath: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    594\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Save the checkpoint to the given filepath.\u001B[39;00m\n\u001B[32m    595\u001B[39m \n\u001B[32m    596\u001B[39m \u001B[33;03m    For manual optimization, we rely on the fact that the model's training_step method saves the model state before\u001B[39;00m\n\u001B[32m    597\u001B[39m \u001B[33;03m    the optimizer step, so we can use that state directly.\u001B[39;00m\n\u001B[32m    598\u001B[39m \n\u001B[32m    599\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m600\u001B[39m     \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msave_weights_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    601\u001B[39m     \u001B[38;5;28mself\u001B[39m._last_global_step_saved = trainer.global_step\n\u001B[32m    602\u001B[39m     \u001B[38;5;28mself\u001B[39m._last_checkpoint_saved = filepath\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1464\u001B[39m, in \u001B[36mTrainer.save_checkpoint\u001B[39m\u001B[34m(self, filepath, weights_only, storage_options)\u001B[39m\n\u001B[32m   1462\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.profiler.profile(\u001B[33m\"\u001B[39m\u001B[33msave_checkpoint\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   1463\u001B[39m     checkpoint = \u001B[38;5;28mself\u001B[39m._checkpoint_connector.dump_checkpoint(weights_only)\n\u001B[32m-> \u001B[39m\u001B[32m1464\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstrategy\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1465\u001B[39m     \u001B[38;5;28mself\u001B[39m.strategy.barrier(\u001B[33m\"\u001B[39m\u001B[33mTrainer.save_checkpoint\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:491\u001B[39m, in \u001B[36mStrategy.save_checkpoint\u001B[39m\u001B[34m(self, checkpoint, filepath, storage_options)\u001B[39m\n\u001B[32m    482\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001B[39;00m\n\u001B[32m    483\u001B[39m \n\u001B[32m    484\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    488\u001B[39m \n\u001B[32m    489\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    490\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_global_zero:\n\u001B[32m--> \u001B[39m\u001B[32m491\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcheckpoint_io\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\lightning_fabric\\plugins\\io\\torch_io.py:57\u001B[39m, in \u001B[36mTorchCheckpointIO.save_checkpoint\u001B[39m\u001B[34m(self, checkpoint, path, storage_options)\u001B[39m\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[32m     52\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     53\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m is not supported for `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m`. Please implement your custom `CheckpointIO`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     54\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m to define how you\u001B[39m\u001B[33m'\u001B[39m\u001B[33md like to use `storage_options`.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     55\u001B[39m     )\n\u001B[32m     56\u001B[39m fs = get_filesystem(path)\n\u001B[32m---> \u001B[39m\u001B[32m57\u001B[39m \u001B[43mfs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmakedirs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdirname\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     58\u001B[39m _atomic_save(checkpoint, path)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\capstone_project_1\\.venv\\Lib\\site-packages\\fsspec\\implementations\\local.py:53\u001B[39m, in \u001B[36mLocalFileSystem.makedirs\u001B[39m\u001B[34m(self, path, exist_ok)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmakedirs\u001B[39m(\u001B[38;5;28mself\u001B[39m, path, exist_ok=\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m     52\u001B[39m     path = \u001B[38;5;28mself\u001B[39m._strip_protocol(path)\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m     \u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmakedirs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen os>:217\u001B[39m, in \u001B[36mmakedirs\u001B[39m\u001B[34m(name, mode, exist_ok)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen os>:217\u001B[39m, in \u001B[36mmakedirs\u001B[39m\u001B[34m(name, mode, exist_ok)\u001B[39m\n",
      "    \u001B[31m[... skipping similar frames: makedirs at line 217 (2 times)]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen os>:217\u001B[39m, in \u001B[36mmakedirs\u001B[39m\u001B[34m(name, mode, exist_ok)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen os>:227\u001B[39m, in \u001B[36mmakedirs\u001B[39m\u001B[34m(name, mode, exist_ok)\u001B[39m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [WinError 161] The specified path is invalid: '///E:/'"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T16:55:45.953879Z",
     "start_time": "2026-02-11T16:55:45.952460Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "261632a831415cd3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
